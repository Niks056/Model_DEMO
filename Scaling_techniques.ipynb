{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f488e39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3803c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"Social_Network_Ads.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed58776f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_data(feature_scaler):\n",
    "    columns_to_scale= list(data.select_dtypes(exclude=[\"object\",\"datetime64\"]).columns)\n",
    "    scaled_array = feature_scaler.fit_transform(data.loc[:,columns_to_scale])\n",
    "    df_scaled = pd.DataFrame(scaled_array,columns=data.select_dtypes(exclude=[\"object\"]).columns)\n",
    "    df_scaled.loc[:,columns_to_scale] = feature_scaler.fit_transform(df_scaled.loc[:,columns_to_scale])\n",
    "    return df_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab6bdae",
   "metadata": {},
   "source": [
    "## Min Max Scaler\n",
    "\n",
    "All features transformed into the range [0,1] meaning that the minimum and maximum value of a feature/variable is going to be 0 and 1, respectively.\n",
    "\n",
    "Xsc=X−Xmin/Xmax−Xmin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6b8011",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "df_scaled=scale_data(scaler)\n",
    "df_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddab490b",
   "metadata": {},
   "source": [
    "## Standard Scaler\n",
    "\n",
    "StandardScaler transforms data such that its distribution will have a mean value 0 and standard deviation 1.\n",
    "\n",
    "z= (x-μ)/σ \n",
    " \n",
    " μ= mean\n",
    " σ= standard deviation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a025a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "df_scaled=scale_data(scaler)\n",
    "df_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d750df",
   "metadata": {},
   "source": [
    "## Normalizer\n",
    "\n",
    "Normalize samples individually to unit norm.\n",
    "Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1, l2 or inf) equals one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5f7a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "scaler = Normalizer(norm = 'l2')\n",
    "# norm = 'l2' is default\n",
    "\n",
    "df_scaled=scale_data(scaler)\n",
    "df_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7906633",
   "metadata": {},
   "source": [
    "## MaxAbs Scaler\n",
    "\n",
    "Maximum absolute scaling scales the data to its maximum value; that is, it divides every observation by the maximum value of the variable\n",
    "\n",
    "x_scaled= x/max(x)\n",
    "\n",
    "The result of the preceding transformation is a distribution in which the values vary approximately within the range of -1 to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa21253",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "scaler = MaxAbsScaler()\n",
    "\n",
    "df_scaled=scale_data(scaler)\n",
    "df_scaled.head()"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAF0AAAAiCAYAAAA9BdXXAAAEpUlEQVRoge2aMZLcNhBF3643N29g+gRmqGwZKjOducqB6BswdKbRDcYn0PgEGp9AVObMuoDLDB2OM2fjoNHDHhCgQHBnt0baX7VVWA6ABj4a3QC64fpRAPfur8xoXwHtAlldhozPBgWwBTauXLjyJqOPJagWyvhsUAI9QoCPHmgS+9mQtzu2me2uFgXwJ3GTsEGIT0FqPR8Vy3fICXe5DZGVLhESeqAGDuRPJBUdcAPsVvZTEx5rgZBaOhm6a/amzkfXPgu3me1UYI+Q0AGDN7BLoWWe8BJZ/E+hRsizUMJ718/elXdMSU6REUQu6SAkK/bIBHwbu1YbfZTAN0zJsrAaXCKLNETq+sQp4YqPro7/3cpajFzzYgdQMZLgTy5md8uZ36wMf6JFQL5F7eroYg+u/PYTsqxM25eeUmKL9iRoODcp6tH1WBU6XazF0fRbejIGwot5DHzbMK+ptk3opNIzKsEi5JiXAnjvyg2jlqvz0fLAZUh/4+SqEwch7z1CZKpJG5iSvkPG3AAfzPfQ4hRk2vWbjDYFot07ZOBK/MDUHuqJ5qGhu+itG88PjM48RMaR6VxL5Nhnz/Qb11YVRsv+AUFNWOp94FFQEfb4l4Jqt2qpj5B50XY5JqLj8eaWDCX9sTThCPzN1MGWwGv3+2umROVc6a2j/qLRINqXo7Uty3xPrpxneEglXR/WnnFtuAF+eupBfGm4A/566kE84xlXjVTnZG+W14IlAYxJ3a8ClQrgR+QYVgEvgD8WDqpBbnP/JNQ9AL8gN9j/FsrJQQO8ROb1wslMGadCH+tS4wYV4WfkE/St2mqefUVMQc3y4O1jXDj0fdxekGwAJhU5MYOOyO5oZzrck05kbiDjUq+SIBOOPcA1yG02BS3pmQO+/IlS6cNUzLamxh3XxA716eAS6Ilf9wvkmSDFTq+JjJ2egjWIsXEdrn0RjNmuxvy+Z7SLNqoTijw9BFokJyb2DqRkpzjzUJ2KMcw3mG8aQFFo9kJ/64TeM69lDdOoir5pW+g2xqurz6MHzmOc/iJf4hm4AX6f6VsX2iqLBt0tYnZfw4Nb4J0rx2TVIJquncecZQF8x2jTdVf0rk3LeSDD34ID5xqghC99Gu2Ar2d+PwC/Br6rs4yhBn4z/zfInLZMFSjEkZ3PBzeOkIk9Oew7xlWJrc4G0RQduF3JA7Joc6cbO/CaPEcEK/JMZlACr4Bvzbc9cTMXMi/KRSylY9L2FiHsXyOoYbR/esa0RGnHr1zbfeA3Cz+EZwcZHdgDQtMpVKYN9b0DfiYt8DwQdrb6zZpgDZBbaCTqFCNtEY3WY6Pmemjw1t8FA6L9FdPAsE+mOkjbT+ykcAmbrilwOs49o195w7KYqk+k5k5WXp0TwQaaCngGjaLon93Olkir9WrfFQVTm67aVbhyTZh0DURcCh0y3o7zSH7oVBPLFNhxPvbStVfNbiLtIOHIXSP5gvfIFrQrbAesWbMWucmVufHKHPTA94iJDC10jHQNZi9F0i1dI+pHpo5P87ljmpmTfrzGweZgi8wtlrXVEQ/j5dycky99a0JTSxPtnyLfe02q8xIz2OLx+D9JXhW12aR4+QAAAABJRU5ErkJggg=="
    }
   },
   "cell_type": "markdown",
   "id": "fa14ec04",
   "metadata": {},
   "source": [
    "## Robust Scaler\n",
    "\n",
    "RobustScaler transforms the feature vector by subtracting the median and then dividing by the interquartile range (75% value — 25% value). \n",
    "\n",
    "It is used to scale features using statistics that are robust to outliers\n",
    "\n",
    "![image.png](attachment:image.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37828183",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "scaler = RobustScaler()\n",
    "df_scaled=scale_data(scaler)\n",
    "df_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c689ed",
   "metadata": {},
   "source": [
    "## Quantile Transformer\n",
    "\n",
    "Quantile Transformation is a non-parametric data transformation technique to transform your numerical data distribution to following a certain data distribution (often the Gaussian Distribution (Normal Distribution)). In the Scikit-Learn, the Quantile Transformer can transform the data into Normal distribution or Uniform distribution; it depends on your distribution references.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac51adf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import QuantileTransformer\n",
    "scaler = QuantileTransformer()\n",
    "df_scaled=scale_data(scaler)\n",
    "df_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d7e195",
   "metadata": {},
   "source": [
    "## Power Transformer\n",
    "\n",
    "While Quantile Transformer is a non-parametric transformer applying Quantile Function, Power Transformer is a parametric transformer via power function. Like the Quantile Transformer, Power Transformer is often used to transform data to follow the Normal Distribution.\n",
    "\n",
    "From Scikit-Learn, two methods are given within the Power Transformer class: Yeo-Johnson transform, and Box-Cox transforms. The basic difference between the methods is the data they allowed to be transformed — Box-Cox needs the data to be positive, while Yeo-Johnson allowed the data to be both negative and positive. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750387d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PowerTransformer\n",
    "scaler = PowerTransformer(method = 'yeo-johnson')\n",
    "'''\n",
    "parameters:\n",
    "method = 'box-cox' or 'yeo-johnson'\n",
    "'''\n",
    "\n",
    "df_scaled=scale_data(scaler)\n",
    "df_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f5bcbc",
   "metadata": {},
   "source": [
    "## Function Transformer\n",
    "\n",
    "Scikit-Learn has provided us many transformation methods that we could use for the data preprocessing pipeline. However, we want to apply our own function for data transformation, but Scikit-Learn did not offer it. That is why Scikit-Learn also presents the Function Transformers class to develop their own data transformation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ff300d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "transformer = FunctionTransformer(np.log2, validate = True)\n",
    "df_scaled=scale_data(transformer)\n",
    "df_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d5f52d",
   "metadata": {},
   "source": [
    "## K-Bins Discretizations\n",
    "\n",
    "Discretization is a process of transforming the continuous feature into a categorical feature by partitioning it into several bins within the expected value range (intervals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d834be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "scaler = KBinsDiscretizer(n_bins = 5, encode = 'ordinal', strategy='quantile')\n",
    "df_scaled=scale_data(scaler)\n",
    "df_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95921516",
   "metadata": {},
   "source": [
    "##  Feature Binarization\n",
    "\n",
    "Feature Binarization is a simple discretization process using a certain threshold to transform the continuous feature into a categorical feature. The value results from Feature Binarization is Boolean value — True or False (0 or 1). Let’s try to use the Binarization class from Scikit-Learn to understand the concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a926b538",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Binarizer\n",
    "\n",
    "#Setting the threshold to 20\n",
    "transformer = Binarizer( threshold = 20)\n",
    "df_scaled=scale_data(transformer)\n",
    "df_scaled.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
