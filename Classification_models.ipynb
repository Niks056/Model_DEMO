{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score,classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the dataset\n",
    "dataset = pd.read_csv('Social_Network_Ads.csv')\n",
    "X = dataset.iloc[:, [2, 3]].values\n",
    "y = dataset.iloc[:, 4].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "It is a classification technique based on an bayes theorm with an assumption of independence among predictors(It assumes\n",
    "that the presence of particular feature in a class is unrealted to the presence of the other feature) Even if these features\n",
    "depends on each other, all of these properties independently contribute to the probability that is why it is known as Naive\n",
    "\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "def Naive_Bayes_model(X_train,y_train):\n",
    "    classifier = GaussianNB()\n",
    "    classifier.fit(X_train, y_train)\n",
    "    return classifier\n",
    "classifier=Naive_Bayes_model(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"criterion{“gini”, “entropy”}, default=”gini”\n",
    "The function to measure the quality of a split. Supported criteria are “gini” for the Gini impurity and “entropy” for the information gain.\n",
    "splitter{“best”, “random”}, default=”best”\n",
    "The strategy used to choose the split at each node. Supported strategies are “best” to choose the best split and “random” to choose the best random split.\n",
    "max_depthint, default=None\n",
    "The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "def Decision_Tree_model(X_train,y_train):\n",
    "    classifier = DecisionTreeClassifier(criterion = 'entropy',random_state = 0)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    return classifier\n",
    "classifier=Decision_Tree_model(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" criterionstring, optional (default=”gini”)\n",
    "The function to measure the quality of a split. Supported criteria are “gini” for the Gini impurity and “entropy” for the information gain. \n",
    "Note: this parameter is tree-specific.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "def Random_Forest_model(X_train,y_train):\n",
    "    classifier = RandomForestClassifier(n_estimators = 10, criterion = 'gini', random_state = 0)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    return classifier\n",
    "classifier=Random_Forest_model(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"kernelstring, optional (default=’rbf’)\n",
    "Specifies the kernel type to be used in the algorithm. It must be one of ‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’ or\n",
    "a callable. If none is given, ‘rbf’ will be used. If a callable is given it is used to pre-compute the kernel matrix from data matrices; \n",
    "that matrix should be an array of shape (n_samples, n_samples).\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting SVM to the Training set\n",
    "from sklearn.svm import SVC\n",
    "def SVM_model(X_train,y_train):\n",
    "    classifier = SVC(kernel = 'linear', random_state = 0)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    return classifier\n",
    "classifier=SVM_model(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "def kernel_svm_model(X_train, y_train):\n",
    "    classifier = SVC(kernel = 'rbf', random_state = 0)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    return classifier\n",
    "classifier=kernel_svm_model(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The K-nearest algorithm is a supervised classification algorithm. It takes a bunch of label points to use them to learn \n",
    "how to label other points. To label a new point, It looks at the labelled points closet to that new point which are its\n",
    "nearest neighbours and has those neighbours vote.\n",
    "\n",
    "1. Determine the value of K\n",
    "2. Calculate the distance between the new point and all the training data.The most commonly used metric are eucledian, \n",
    "manhattan and Minoswki\n",
    "3. Sort the distance and determine K-nearest neighbours based on min distance.\n",
    "4. Analyze the Category of those neighbour and assign acc to majority votes\n",
    "5. Return the predicted class.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "def knn_model(X_train, y_train):\n",
    "    classifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    return classifier\n",
    "classifier=knn_model(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Catboost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "“CatBoost” name comes from two words “Category” and “Boosting”.\n",
    "CatBoost can use categorical features directly and is scalable in nature.The library works well with multiple Categories \n",
    "of data, such as audio, text, image including historical data.\n",
    "“Boost” comes from gradient boosting machine learning algorithm as this library is based on gradient boosting library.\n",
    "Gradient boosting is a powerful machine learning algorithm that is widely applied to multiple types of business \n",
    "challenges like fraud detection, recommendation items, forecasting and it performs well also. It can also return very\n",
    "good result with relatively less data, unlike DL models that need to learn from a massive amount of data.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "def catboost_classifier(X_train,y_train):\n",
    "    clf = CatBoostClassifier(\n",
    "        iterations=10,\n",
    "    #     verbose=5)\n",
    "    #CatBoostClassifier(task_type=\"GPU\",devices='0:1')\n",
    "\n",
    "    clf.fit(\n",
    "        X_train, y_train,\n",
    "        cat_features=cat_features,\n",
    "        eval_set=(X_val, y_val))\n",
    "    return clf\n",
    "classifier=catboost_classifier(X_train,y_train,X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Gradient Boosting is an extension over boosting method. An ensemble of tree is built one by one and individually trees \n",
    "are summed sequentially.In this algorithm new tree learns sequentially from the previous tree that fits realtively \n",
    "simple models to the data. We fit Consequtive trees at every step and the goal is to reduce the error from the previous \n",
    "tree. If an input is wrongly Interpreted, its weight is increased so that next step classifies it correctly. By combining \n",
    "the whole set at the end and converts weak learners in to a better model. \n",
    "\"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "def gradient_boosting_classifier(X_train,y_train):\n",
    "    gbc=GradientBoostingClassifier(n_estimators=500,learning_rate=0.05,random_state=100,max_features=5 )\n",
    "    gbc.fit(X_train,y_train)\n",
    "    return gbc\n",
    "classifier=gradient_boosting_classifier(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LGBM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "LightGBM is a gradient boosting framework that uses tree based learning algorithm.LightGBM grows tree vertically while\n",
    "other tree based learning algorithms grow trees horizontally. It means that LightGBM grows tree leaf-wise while other \n",
    "algorithms grow level-wise. It will choose the leaf with max delta loss to grow. When growing the same leaf, \n",
    "leaf-wise algorithm can reduce more loss than a level-wise algorithm.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "def lgb_classifier():\n",
    "    clf = lgb.LGBMClassifier()\n",
    "    clf.fit(X_train, y_train)\n",
    "    return clf\n",
    "classifier=lgb_classifier(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost Vs LightGBM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "XGBoost is a very fast and accurate ML algorithm. But now it's been challenged by LightGBM — which runs even faster \n",
    "with comparable model accuracy and more hyperparameters for users to tune.The key difference in speed is because\n",
    "XGBoost split the tree nodes one level at a time and LightGBM does that one node at a time.So XGBoost developers \n",
    "later improved their algorithms to catch up with LightGBM, allowing users to also run XGBoost in split-by-leaf mode \n",
    "(grow_policy = ‘lossguide’). Now XGBoost is much faster with this improvement, but LightGBM is still about 1.3X — 1.5X\n",
    "the speed of XGB.Another difference between XGBoost and LightGBM is that XGBoost has a feature that LightGBM lacks — \n",
    "monotonic\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ada Boost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Ada Boosting is general ensemble method that keeps adding weak learner to correct classificaton errors and creates a \n",
    "strong classifier from a number of weak learners. This is done by building a model from training data then creating a \n",
    "second model that attempts to correct the errors from the first model. Models are added until the training set is predicted\n",
    "perfectly or a maximum number of models are added\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "def adaboost_classifer():\n",
    "    Abc=AdaBoostClassifier()\n",
    "    Abc.fit(X_train,y_train)\n",
    "    return Abc\n",
    "classifier=adaboost_classifer(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model_Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_metrics(classifier,X_test,y_test):\n",
    "    \n",
    "    # Predicting the Test set results\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    \n",
    "    # Making the Confusion Matrix\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    \n",
    "    # Accuracy Score\n",
    "    print(accuracy_score(y_test, y_pred))\n",
    "    \n",
    "    #Classification Report\n",
    "    print(classification_report(y_test, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
