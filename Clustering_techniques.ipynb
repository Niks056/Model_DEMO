{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.cluster import Birch\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import SpectralClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_process_df=pd.read_csv(\"data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SCALED\n",
    "scaler = StandardScaler()\n",
    "scaled_df = scaler.fit_transform(df.iloc[:,1:])\n",
    "scaled_df = pd.DataFrame(data=scaled_df,columns=df.iloc[:,1:].columns)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def clustering(clustering_method,n_clusters,df):\n",
    "    labels =  clustering_method(n_clusters = n_clusters, random_state=0).fit_predict(df)\n",
    "    sil_score = metrics.silhouette_score(df, labels)\n",
    "    return (labels, sil_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_clustering(df, n_clusters):\n",
    "    labels =  KMeans(n_clusters = n_clusters, random_state=0).fit_predict(df)\n",
    "    sil_score = metrics.silhouette_score(df, labels)\n",
    "    return (labels, sil_score)\n",
    "\n",
    "def kmedoids_clustering(df, n_clusters):\n",
    "    kmds_model= KMedoids(n_clusters=n_clusters,random_state=0)\n",
    "    labels = kmds_model.fit_predict(df)\n",
    "    sil_score = metrics.silhouette_score(df, labels)\n",
    "    db_index=  metrics.davies_bouldin_score(df, labels)\n",
    "    #dunn_score= base.dunn(list(df['KMEDOIDS_CLUSTER_NO']))\n",
    "    return(labels,sil_score,db_index)\n",
    "\n",
    "def agglomerative_clustering(df, n_clusters):\n",
    "    agg_model =  AgglomerativeClustering(n_clusters = n_clusters)\n",
    "    labels = agg_model.fit_predict(df)\n",
    "    sil_score = metrics.silhouette_score(df, labels)\n",
    "    db_index=  metrics.davies_bouldin_score(df,labels)\n",
    "    return (labels,sil_score,db_index)\n",
    "\n",
    "def birch_clustering(df, n_clusters):\n",
    "    birch_model = Birch(n_clusters=n_clusters)\n",
    "    labels= birch_model.fit_predict(df)\n",
    "    sil_score = metrics.silhouette_score(df, labels)\n",
    "    db_index=  metrics.davies_bouldin_score(df,labels)\n",
    "    return (labels,sil_score,db_index)\n",
    "\n",
    "def Gaussian_Modelling(df,n_clusters):\n",
    "    gm_model = GaussianMixture(n_components=n_clusters, random_state=0)\n",
    "    labels=gm_model.fit_predict(df)\n",
    "    sil_score = metrics.silhouette_score(df, labels)\n",
    "    db_index=  metrics.davies_bouldin_score(df,labels)\n",
    "    return (labels,sil_score,db_index) ### K Medoids Clustering\n",
    "\n",
    "def dbscan_clustering(df, r,ms):\n",
    "    dbscan_model = DBSCAN(eps=r, min_samples=ms)\n",
    "    labels= dbscan_model.fit_predict(df)\n",
    "    sil_score = metrics.silhouette_score(df, labels)\n",
    "    db_index=  metrics.davies_bouldin_score(df,labels)\n",
    "    return(labels,sil_score,db_index)\n",
    "\n",
    "def spectral_clustering(df,n_clusters):\n",
    "    spectral_model_rbf = SpectralClustering(n_clusters = n_clusters, affinity ='rbf')\n",
    "    labels_rbf = spectral_model_rbf.fit_predict(df)\n",
    "    sil_score = metrics.silhouette_score(df, labels)\n",
    "    db_index=  metrics.davies_bouldin_score(df,labels)\n",
    "    return(labels,sil_score,db_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K means Custering\n",
    "\n",
    "The K-means clustering algorithm computes centroids and repeats until the optimal centroid is found. It is presumptively known how many clusters there are. It is also known as the flat clustering algorithm. The number of clusters found from data by the method is denoted by the letter ‘K’ in K-means.\n",
    "\n",
    "1. First, we need to provide the number of clusters, K, that need to be generated by this algorithm.\n",
    "2. Next, choose K data points at random and assign each to a cluster. Briefly, categorize the data based on the number of data points.\n",
    "3. The cluster centroids will now be computed.\n",
    "4. Iterate the steps below until we find the ideal centroid, which is the assigning of data points to clusters that do not vary.\n",
    "\n",
    "4.1. The sum of squared distances between data points and centroids would be calculated first.\n",
    "4.2. At this point, we need to allocate each data point to the cluster that is closest to the others (centroid).\n",
    "4.3. Finally, compute the centroids for the clusters by averaging all of the cluster’s data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sil_scores = [kmeans_clustering(scaled_df, i)[1] for i in range(2,12)]\n",
    "db_index =   [kmeans_clustering(scaled_df, i)[2] for i in range(2,12)]\n",
    "print (\"Max Sil score:\",max(sil_scores))\n",
    "print (\"Min DB index:\",min(db_index))\n",
    "scaled_df['cluster_no_db']=kmeans_clustering(scaled_df,db_index.index(min(db_index))+2)[0]\n",
    "scaled_df['cluster_no_sil']=kmeans_clustering(scaled_df,sil_scores.index(max(sil_scores))+2)[0]\n",
    "print (\"DB Clusters:\\n\",scaled_df['cluster_no_db'].value_counts())\n",
    "print(\"Sil Clusters:\\n\",scaled_df['cluster_no_sil'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Medoids Clustering\n",
    "\n",
    "The steps followed by the K-Medoids algorithm for clustering are as follows:\n",
    "\n",
    "1. Randomly choose ‘k’ points from the input data (‘k’ is the number of clusters to be formed). The correctness of the choice of k’s value can be assessed using methods such as silhouette method.\n",
    "\n",
    "2. Each data point gets assigned to the cluster to which its nearest medoid belongs.\n",
    "\n",
    "3. For each data point of cluster i, its distance from all other data points is computed and added. The point of ith cluster for which the computed sum of distances from other points is minimal is assigned as the medoid for that cluster.\n",
    "\n",
    "4. Steps (2) and (3) are repeated until convergence is reached i.e. the medoids stop moving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sil_scores = [kmedoids_clustering(scaled_df, i)[1] for i in range(2,12)]\n",
    "db_index =   [kmedoids_clustering(scaled_df, i)[2] for i in range(2,12)]\n",
    "print (\"Max Sil score:\",max(sil_scores))\n",
    "print (\"Min DB index:\",min(db_index))\n",
    "scaled_df['cluster_no_db']=kmedoids_clustering(scaled_df,db_index.index(min(db_index))+2)[0]\n",
    "scaled_df['cluster_no_sil']=kmedoids_clustering(scaled_df,sil_scores.index(max(sil_scores))+2)[0]\n",
    "print (\"DB Clusters:\\n\",scaled_df['cluster_no_db'].value_counts())\n",
    "print(\"Sil Clusters:\\n\",scaled_df['cluster_no_sil'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agglomerative_clustering\n",
    "\n",
    "1. Initially, all the data-points are a cluster of its own.\n",
    "2. Take two nearest clusters and join them to form one single cluster.\n",
    "3. Proceed recursively step 2 until you obtain the desired number of clusters.\n",
    "\n",
    "There are some methods which are used to calculate the similarity between two clusters:\n",
    "\n",
    "1. Distance between two closest points in two clusters.\n",
    "2. Distance between two farthest points in two clusters.\n",
    "3. The average distance between all points in the two clusters.\n",
    "4. Distance between centroids of two clusters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sil_scores = [agglomerative_clustering(scaled_df, i)[1] for i in range(2,12)]\n",
    "db_index =   [agglomerative_clustering(scaled_df, i)[2] for i in range(2,12)]\n",
    "print (\"Max Sil score:\",max(sil_scores))\n",
    "print (\"Min DB index:\",min(db_index))\n",
    "scaled_df['cluster_no_db']=agglomerative_clustering(scaled_df,db_index.index(min(db_index))+2)[0]\n",
    "scaled_df['cluster_no_sil']=agglomerative_clustering(scaled_df,sil_scores.index(max(sil_scores))+2)[0]\n",
    "print (\"DB Clusters:\\n\",scaled_df['cluster_no_db'].value_counts())\n",
    "print(\"Sil Clusters:\\n\",scaled_df['cluster_no_sil'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Birch_clustering\n",
    "\n",
    "Clustering using Hierarchies (BIRCH) is a clustering algorithm that can cluster large datasets by first generating a small and compact summary of the large dataset that retains as much information as possible. This smaller summary is then clustered instead of clustering the larger dataset.\n",
    "\n",
    "His maximum number is called the threshold. We will learn more about what this threshold value is. Parameters of BIRCH Algorithm :\n",
    "\n",
    "1. threshold :  threshold is the maximum number of data points a sub-cluster in the leaf node of the CF tree can hold.\n",
    "2. branching_factor : This parameter specifies the maximum number of CF sub-clusters in each node (internal node).\n",
    "3. n_clusters : The number of clusters to be returned after the entire BIRCH algorithm is complete i.e., number of clusters after the final clustering step. If set to None, the final clustering step is not performed and intermediate clusters are returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sil_scores = [birch_clustering(scaled_df, i)[1] for i in range(2,12)]\n",
    "db_index =   [birch_clustering(scaled_df, i)[2] for i in range(2,12)]\n",
    "print (\"Max Sil score:\",max(sil_scores))\n",
    "print (\"Min DB index:\",min(db_index))\n",
    "scaled_df['cluster_no_db']=birch_clustering(scaled_df,db_index.index(min(db_index))+2)[0]\n",
    "scaled_df['cluster_no_sil']=birch_clustering(scaled_df,sil_scores.index(max(sil_scores))+2)[0]\n",
    "print (\"DB Clusters:\\n\",scaled_df['cluster_no_db'].value_counts())\n",
    "print(\"Sil Clusters:\\n\",scaled_df['cluster_no_sil'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Mixture Model\n",
    "\n",
    "Gaussian Mixture Models (GMMs) assume that there are a certain number of Gaussian distributions, and each of these distributions represent a cluster\n",
    "let’s say we have three Gaussian distributions (more on that in the next section) – GD1, GD2, and GD3. These have a certain mean (μ1, μ2, μ3) and variance (σ1, σ2, σ3) value respectively. For a given set of data points, our GMM would identify the probability of each data point belonging to each of these distributions.\n",
    "\n",
    "Gaussian Mixture Models are probabilistic models and use the soft clustering approach for distributing the points in different clusters\n",
    "\n",
    "Expectation-Maximization (EM) is a statistical algorithm for finding the right model parameters. \n",
    " the Expectation-Maximization algorithm has two steps:\n",
    "\n",
    "E-step: In this step, the available data is used to estimate (guess) the values of the missing variables\n",
    "M-step: Based on the estimated values generated in the E-step, the complete data is used to update the parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sil_scores = [Gaussian_Modelling(scaled_df, i)[1] for i in range(2,12)]\n",
    "db_index =   [Gaussian_Modelling(scaled_df, i)[2] for i in range(2,12)]\n",
    "print (\"Max Sil score:\",max(sil_scores))\n",
    "print (\"Min DB index:\",min(db_index))\n",
    "scaled_df['cluster_no_db']=Gaussian_Modelling(scaled_df,db_index.index(min(db_index))+2)[0]\n",
    "scaled_df['cluster_no_sil']=Gaussian_Modelling(scaled_df,sil_scores.index(max(sil_scores))+2)[0]\n",
    "print (\"DB Clusters:\\n\",scaled_df['cluster_no_db'].value_counts())\n",
    "print(\"Sil Clusters:\\n\",scaled_df['cluster_no_sil'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBSCAN Clustering\n",
    "\n",
    "DBSCAN stands for Density-Based Spatial Clustering of Applications with Noise.\n",
    "\n",
    "DBSCAN is a density-based clustering algorithm that works on the assumption that clusters are dense regions in space separated by regions of lower density. \n",
    "\n",
    "It groups ‘densely grouped’ data points into a single cluster. It can identify clusters in large spatial datasets by looking at the local density of the data points. The most exciting feature of DBSCAN clustering is that it is robust to outliers. It also does not require the number of clusters to be told beforehand, unlike K-Means, where we have to specify the number of centroids.\n",
    "\n",
    "DBSCAN requires only two parameters: epsilon and minPoints. Epsilon is the radius of the circle to be created around each data point to check the density and minPoints is the minimum number of data points required inside that circle for that data point to be classified as a Core point.\n",
    "\n",
    "DBSCAN creates a circle of epsilon radius around every data point and classifies them into Core point, Border point, and Noise. A data point is a Core point if the circle around it contains at least ‘minPoints’ number of points. If the number of points is less than minPoints, then it is classified as Border Point, and if there are no other data points around any data point within epsilon radius, then it treated as Noise.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sil_scores = [dbscan_clustering(data,i,j)[1] for i in np.arange(0.1,0.5,0.01) for j in range(2,6)]\n",
    "db_index =  [dbscan_clustering(data,i,j)[2] for i in range(2,12)]\n",
    "print (\"Max Sil score:\",max(sil_scores))\n",
    "print (\"Min DB index:\",min(db_index))\n",
    "data['dbscan_cluster_no_db']=dbscan_clustering(data,db_index.index(min(db_index))+2)[0]\n",
    "data['dbscan_cluster_no_sil']=dbscan_clustering(data,sil_scores.index(max(sil_scores))+2)[0]\n",
    "print (\"DB Clusters:\\n\",data['dbscan_cluster_no_db'].value_counts())\n",
    "print(\"Sil Clusters:\\n\",data['dbscan_cluster_no_sil'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spectral clustering (graph distance)\n",
    "\n",
    "Spectral clustering treats each data point as a graph-node and thus transforms the clustering problem into a graph-partitioning problem.\n",
    "\n",
    "The three major steps involved in spectral clustering are: constructing a similarity graph, projecting data onto a lower-dimensional space, and clustering the data. Given a set of points S in a higher-dimensional space, it can be elaborated as follows:\n",
    "\n",
    "1. Form a distance matrix\n",
    "2. Transform the distance matrix into an affinity matrix A\n",
    "3. Compute the degree matrix D and the Laplacian matrix L = D – A.\n",
    "4. Find the eigenvalues and eigenvectors of L.\n",
    "5. With the eigenvectors of k largest eigenvalues computed from the previous step form a matrix.\n",
    "6. Normalize the vectors.\n",
    "7. Cluster the data points in k-dimensional spac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sil_scores = [spectral_clustering(scaled_df, i)[1] for i in range(2,12)]\n",
    "db_index =   [spectral_clustering(scaled_df, i)[2] for i in range(2,12)]\n",
    "print (\"Max Sil score:\",max(sil_scores))\n",
    "print (\"Min DB index:\",min(db_index))\n",
    "scaled_df['cluster_no_db']=spectral_clustering(scaled_df,db_index.index(min(db_index))+2)[0]\n",
    "scaled_df['cluster_no_sil']=spectral_clustering(scaled_df,sil_scores.index(max(sil_scores))+2)[0]\n",
    "print (\"DB Clusters:\\n\",scaled_df['cluster_no_db'].value_counts())\n",
    "print(\"Sil Clusters:\\n\",scaled_df['cluster_no_sil'].value_counts())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
