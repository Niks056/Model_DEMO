# Importing the libraries
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix,accuracy_score,classification_report

# Importing the dataset
dataset = pd.read_csv('Social_Network_Ads.csv')
X = dataset.iloc[:, [2, 3]].values
y = dataset.iloc[:, 4].values

# Splitting the dataset into the Training set and Test set
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)

# Feature Scaling
def feature_Scaling(X_train,X_tes):
    sc = StandardScaler()
    X_train = sc.fit_transform(X_train)
    X_test = sc.transform(X_test)
   
from sklearn.linear_model import LogisticRegression
# Fitting Logistic Regression to the Training set
def logistic_regression_model(X_train,y_train):
    classifier = LogisticRegression(random_state = 0)
    classifier.fit(X_train, y_train)
    return classifier
    
from sklearn.naive_bayes import GaussianNB
def Naive_Bayes_model(X_train,y_train):
    classifier = GaussianNB()
    classifier.fit(X_train, y_train)
    return classifier
    
from sklearn.tree import DecisionTreeClassifier
def Random_Forest_model(X_train,y_train):
    classifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)
    classifier.fit(X_train, y_train)
    return classifier
"""criterion{“gini”, “entropy”}, default=”gini”
The function to measure the quality of a split. Supported criteria are “gini” for the Gini impurity and “entropy” for the information gain.
splitter{“best”, “random”}, default=”best”
The strategy used to choose the split at each node. Supported strategies are “best” to choose the best split and “random” to choose the best random split.
max_depthint, default=None
The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples
"""

from sklearn.ensemble import RandomForestClassifier
def Random_Forest_model(X_train,y_train):
    classifier = RandomForestClassifier(n_estimators = 10, criterion = 'gini', random_state = 0)
    classifier.fit(X_train, y_train)
    return classifier

"""criterionstring, optional (default=”gini”)
The function to measure the quality of a split. Supported criteria are “gini” for the Gini impurity and “entropy” for the information gain. 
Note: this parameter is tree-specific."""

# Fitting SVM to the Training set
from sklearn.svm import SVC
def SVM_model(X_train,y_train):
    classifier = SVC(kernel = 'linear', random_state = 0)
    classifier.fit(X_train, y_train)
    return classifier
    
from sklearn.svm import SVC
def kernel_svm_model(X_train, y_train):
    classifier = SVC(kernel = 'rbf', random_state = 0)
    classifier.fit(X_train, y_train)
    return classifier

"""kernelstring, optional (default=’rbf’)
Specifies the kernel type to be used in the algorithm. It must be one of ‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’ or
a callable. If none is given, ‘rbf’ will be used. If a callable is given it is used to pre-compute the kernel matrix from data matrices; 
that matrix should be an array of shape (n_samples, n_samples)."""

from sklearn.neighbors import KNeighborsClassifier
def knn_model(X_train, y_train):
    classifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)
    classifier.fit(X_train, y_train)
    return classifier
    
def model_metrics(classifier,X_test,y_test):
    
    # Predicting the Test set results
    y_pred = classifier.predict(X_test)
    
    # Making the Confusion Matrix
    print(confusion_matrix(y_test, y_pred))
    
    # Accuracy Score
    print(accuracy_score(y_test, y_pred))
    
    #Classification Report
    print(classification_report(y_test, y_pred))
